import asyncio, os
from dotenv import load_dotenv

from llama_index.llms.openai import OpenAI
from llama_index.core.agent.workflow import ReActAgent, AgentStream, ToolCallResult
from llama_index.core.tools import FunctionTool
from llama_index.core.workflow import Context

# ========= Env =========
load_dotenv()
assert os.environ.get("OPENAI_API_KEY"), "Set OPENAI_API_KEY in your .env"

# ========= Tools =========
def multiply(a: float, b: float) -> float:
    """Multiply two numbers and return the product."""
    return a * b

def verify_product(a: float, b: float) -> str:
    """Recompute the product to verify the prior result; returns a short message."""
    return f"Verified: {a*b}"

def compare_two(a: float, b: float) -> str:
    """
    Compare two numbers and return which one is greater, or if they are equal.
    """
    if a > b:
        return f"{a} is greater than {b}"
    elif a < b:
        return f"{b} is greater than {a}"
    else:
        return f"{a} and {b} are equal"


multiply_tool = FunctionTool.from_defaults(
    fn=multiply, name="multiply", description="Multiply two numbers."
)

verify_tool = FunctionTool.from_defaults(
    fn=verify_product, name="verify_product",
    description="Recompute the product to confirm the result."
)

compare_two_tool = FunctionTool.from_defaults(
    fn=compare_two,
    name="compare_two",
    description="Compare two numbers and return which one is greater or if they are equal."
)

# ========= Agent =========
llm = OpenAI(model="gpt-4o-mini", temperature=0)
agent = ReActAgent(tools=[multiply_tool, verify_tool, compare_two_tool], llm=llm)
ctx = Context(agent)  # maintain state/history across steps

async def main():
    # Nudge the agent to be thorough, but don't force a fixed number of steps.
    q = (
        #"Compute 1234 * 4567. If helpful, call tools and verify before answering. "
        "Which is bigger: 123 * 431, 801 * 45, or 391 * 101?"
        "Be thorough and only give the final answer after youâ€™re confident."
    )

    # Start the run; do NOT pass max_iterations (no artificial cap).
    handler = agent.run(q, ctx=ctx)

    # Stream events as they happen:
    async for ev in handler.stream_events():
        if isinstance(ev, ToolCallResult):
            # Shows which tool ran, with args and output
            print(f"\n[TOOL] {ev.tool_name}({ev.tool_kwargs}) -> {ev.tool_output.raw_output}")
        elif isinstance(ev, AgentStream):
            # Live agent output (tokens). Good for progress feedback.
            # (Some models may not stream; LlamaIndex handles that.)
            print(ev.delta, end="", flush=True)

    # Final response object (includes tool_calls for post-hoc inspection)
    response = await handler
    print("\n\nFINAL:", response.response)
    print("TOOL CALLS:", response.tool_calls)

if __name__ == "__main__":
    asyncio.run(main())

